
#### **1. Контекст проекта**
- **Цель:** Парсинг вакансий с hh.ru по API, проверка их актуальности, хранение в SQLite и подготовка к отправке через собственное API.
- **Стек:** Python 3.10+, `httpx` (асинхронные запросы), SQLite, FastAPI (в перспективе).
- **Источник данных:** Ссылки на вакансии в формате `https://api.hh.ru/vacancies/{ID}?host=hh.ru`.
- **Частота обновления:** Ежедневная утренняя проверка (23:30 по МСК).
- **Ограничения API hh.ru:** Пауза 1-2 секунды между запросами.

#### **2. Структура базы данных (SQLite)**
Понятно! Вот скорректированный раздел про **структуру базы данных** с учётом того, что **уникальными должны быть сами ссылки**, а не только ID:

---

### **2. Структура базы данных (SQLite)**

#### **Таблица `vacancy_links`:**

| Поле          | Тип        | Описание                          | Ограничения                     |
|---------------|------------|-----------------------------------|---------------------------------|
| `id`          | INTEGER    | Уникальный ID вакансии            | `PRIMARY KEY`                   |
| `url`         | TEXT       | Полная ссылка на вакансию         | `UNIQUE` (гарантия уникальности)|
| `is_active`   | BOOLEAN    | Статус вакансии (`True` — активна) |                                 |
| `last_checked`| DATETIME   | Дата последней проверки           |                                 |

#### **Ключевые моменты:**
- **Уникальность `url`:** Поле `url` должно быть уникальным (`UNIQUE`), чтобы избежать дублирования ссылок.
- **Связь с `id`:** `id` извлекается из `url` (например, для `https://api.hh.ru/vacancies/125616194?host=hh.ru` это `125616194`).
- **Индексы:** Рекомендуется создать индекс по `url` для ускорения поиска.

---

### **Пример SQL для создания таблицы:**
```sql
CREATE TABLE vacancy_links (
    id INTEGER PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    last_checked DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### **3. Логика работы**
- **Обход ссылок:**
  - Использовать асинхронные запросы (`httpx` + `asyncio`) для ускорения.
  - Пауза 1-2 секунды между запросами (избегать блокировки API).
  - Проверять поле `archived` в ответе API: если `True` — вакансия закрыта.
- **Хранение данных:**
  - Уникальные ID вакансий (избегать дубликатов).
  - Удалять закрытые вакансии из базы или помечать как неактивные.
- **Источник ID вакансий:**
  - Пока локальный список, позже — из SQLite.
- **Логирование:**
  - Сохранять ошибки запросов (например, 429 Too Many Requests) для отладки.

#### **4. Требования к коду**
- **Формат функций:**
  ```python
  def название_функции():
      pass
      # TODO: Описание задачи (если не просят код напрямую).
  ```
- **Асинхронность:**
  - Все сетевые запросы — асинхронные.
  - Обработка ошибок: ретрай с задержкой при временных сбоях.
- **SQLite:**
  - Создать таблицу `vacancy_links` при первом запуске.
  - Обеспечить уникальность `id` и `url`.
- **Дополнительные задачи:**
  - В будущем: отправка данных через API (спецификация будет позже).

#### **5. Примеры кода (если запрашивается реализация)**
- **Асинхронный обход:**
  ```python
  async def fetch_vacancy(client: httpx.AsyncClient, vacancy_id: int) -> dict:
      url = f"https://api.hh.ru/vacancies/{vacancy_id}?host=hh.ru"
      response = await client.get(url)
      response.raise_for_status()
      return response.json()
  ```
- **Паузы:**
  ```python
  await asyncio.sleep(1)  # Пауза между запросами 1-2 sec
  ```

#### **6. Важные замечания**
- **Не использовать AI для логики:** Только для шаблонов, проверки ошибок или антипаттернов.
- **Сохранение данных:** Построчно, без перезаписи существующих записей.
- **Вопросы:** Задавать только по критичным деталям (например, изменение структуры базы или API).

#### **7 requirements.txt
anyio==4.11.0
certifi==2025.8.3
charset-normalizer==3.4.3
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.10
iniconfig==2.1.0
packaging==25.0
pluggy==1.6.0
Pygments==2.19.2
pytest==8.4.2
pytest-mock==3.15.1
python-dotenv==1.1.1
requests==2.32.5
sniffio==1.3.1
typing_extensions==4.15.0
urllib3==2.5.0
